{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24f67003-99e6-4a7e-87b8-cb305d11ba6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/cafe/u/dmendo/.cache'\n",
    "os.environ['HF_HOME'] = '/cafe/u/dmendo/.cache'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32b167e6-c48d-4aa9-a39a-713888efb6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "535fb4ca-f623-4ce0-8f0e-a43b29493635",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cafe/u/dmendo/logics_home/logics_py_env/lib/python3.8/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from utils import *\n",
    "import openai\n",
    "from tqdm import tqdm\n",
    "openai.api_key = \"INSERT_KEY\"\n",
    "import itertools\n",
    "from matplotlib import pyplot as plt\n",
    "import context_retriever\n",
    "import re\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "import spot_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86feff6e-b4fc-4e8a-a476-c7eaaab2a3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35e9c4e8-fd2f-4199-a769-b512fc1c01f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#duplicate from synthTL\n",
    "def get_literal_node_id(node):\n",
    "    return node.assert_text\n",
    "\n",
    "def get_abstract_node_id(node,ret_var_map=False):\n",
    "    str_dcmp_dict = dict((k,v.assert_text) for k,v in node.dcmp_dict.items())\n",
    "    return get_abstract_node_id_from_dcmpdict(node.assert_text,str_dcmp_dict,ret_var_map=ret_var_map)\n",
    "\n",
    "def remove_duplicate_fromlist_keeporder(seq):\n",
    "    seen = set()\n",
    "    seen_add = seen.add\n",
    "    return [x for x in seq if not (x in seen or seen_add(x))]\n",
    "\n",
    "def get_abstract_node_id_from_dcmpdict(assert_text,dcmp_dict,ret_var_map=False):\n",
    "    abs_text = assert_text\n",
    "    for dcmp_var,dcmp_node_text in dcmp_dict.items():\n",
    "        abs_text = abs_text.replace(dcmp_node_text,\"_\"+dcmp_var+\"-SYMBOL_\")\n",
    "    return get_abstract_node_id_from_abstext(abs_text,ret_var_map=ret_var_map)\n",
    "\n",
    "def normalize_dcmp_dict(assert_text,dcmp_dict):\n",
    "    node_id,var_map = get_abstract_node_id_from_dcmpdict(assert_text,dcmp_dict,ret_var_map=True)\n",
    "    new_dict = {}\n",
    "    for prev_var,new_var in var_map.items():\n",
    "        this_prev_var = prev_var[1:-1] #remove underscores\n",
    "        this_new_var = new_var[1:-1]\n",
    "        new_dict[this_new_var] = dcmp_dict[this_prev_var]\n",
    "    return new_dict\n",
    "\n",
    "def get_abstract_node_id_from_abstext(abs_text,ret_var_map=False,input_mode=False):\n",
    "    if not input_mode:\n",
    "        abs_var_list = remove_duplicate_fromlist_keeporder(re.findall('_[a-zA-Z0-9_]*-SYMBOL_',abs_text))\n",
    "    else:\n",
    "        abs_var_list = remove_duplicate_fromlist_keeporder(re.findall('_[a-zA-Z0-9_]*SYMBOL_',abs_text))\n",
    "    new_abs_var_list = [\"_SYMBOL\"+str(i)+\"_\" for i in range(len(abs_var_list))]\n",
    "    node_id = abs_text\n",
    "    for i in range(len(abs_var_list)):\n",
    "        new_abs_var = \"_SYMBOL\"+str(i)+\"_\"\n",
    "        node_id = node_id.replace(abs_var_list[i],new_abs_var)\n",
    "    if not ret_var_map:\n",
    "        return node_id    \n",
    "    else:\n",
    "        var_map = dict((abs_var_list[i].replace(\"-SYMBOL_\",\"_\"),new_abs_var_list[i]) for i in range(len(abs_var_list)))\n",
    "        return node_id, var_map\n",
    "        \n",
    "def get_unique_node_id(node):\n",
    "    if node.parent is not None:\n",
    "        return get_unique_node_id(node.parent) + \";\" + node.assert_text\n",
    "    else:\n",
    "        return node.assert_text\n",
    "\n",
    "def create_decompose_dict(fname):\n",
    "    decompose_df = pd.read_excel(fname)\n",
    "    decompose_dict = {}\n",
    "    for idx,row in decompose_df.iterrows():\n",
    "        dcmp_dict = json.loads(row['Decomposition'])\n",
    "        dcmp_dict = dict((k,v) for k,v in dcmp_dict.items() if v != \"\" and v != row['Natural language'])\n",
    "        dcmp_dict = normalize_dcmp_dict(row['Natural language'],dcmp_dict)\n",
    "        decompose_dict[row['Natural language']] = dcmp_dict\n",
    "    return decompose_dict\n",
    "\n",
    "def create_translate_dict(fname):\n",
    "    translate_df = pd.read_excel(fname)\n",
    "    translate_dict = {}\n",
    "    for idx,row in translate_df.iterrows():\n",
    "        translate_dict[row['Natural language']] = row['LTL']\n",
    "        if 'Decomposed Natural language' in row and 'Template' in row:\n",
    "            assert \"SYMBOL_\" in row['Decomposed Natural language']\n",
    "            abs_node_id,var_map = get_abstract_node_id_from_abstext(row['Decomposed Natural language'],ret_var_map=True,input_mode=True)\n",
    "            cur_template = row['Template']\n",
    "            for prev_var,new_var in var_map.items():\n",
    "                cur_template = cur_template.replace(prev_var,new_var)\n",
    "            translate_dict[abs_node_id] = cur_template\n",
    "            #translate_dict[row['Decomposed Natural language']] = row['Template']\n",
    "    return translate_dict\n",
    "\n",
    "class Node:\n",
    "    def __init__(self,assert_text,parent=None,translate_fewshots=[],decompose_fewshots=[]):\n",
    "        self.assert_text = assert_text\n",
    "        self.translation = None\n",
    "        self.translation_type = None\n",
    "        self.template_translation = None\n",
    "        self.parent = parent\n",
    "        self.dcmp_dict = {}\n",
    "        self.translate_fewshots = translate_fewshots\n",
    "        self.decompose_fewshots = decompose_fewshots\n",
    "    \n",
    "    def set_dcmp_dict(self,dcmp_str_dict,force_new=True):\n",
    "        if not force_new and dcmp_str_dict == dict((k,v.assert_text) for k,v in self.dcmp_dict.items()):\n",
    "            return\n",
    "        dcmp_str_dict = normalize_dcmp_dict(self.assert_text,dcmp_str_dict)\n",
    "        self.dcmp_dict = {}\n",
    "        for dcmp_var,dcmp_str in dcmp_str_dict.items():\n",
    "            new_node = Node(assert_text=dcmp_str,\n",
    "                            parent=self,\n",
    "                            translate_fewshots=self.translate_fewshots,\n",
    "                            decompose_fewshots=self.decompose_fewshots\n",
    "                        )\n",
    "            self.dcmp_dict[dcmp_var] = new_node\n",
    "\n",
    "    def decompose(self,mode='LLM',**kwargs):\n",
    "        if mode == 'LLM':\n",
    "            dcmp_str_dict = decompose_LLM(self,**kwargs)\n",
    "        elif mode == 'cache':\n",
    "            dcmp_str_dict = get_decompose_cache(self)\n",
    "        else:\n",
    "            assert False, \"decomposition mode not found! \" + mode\n",
    "        self.set_dcmp_dict(dcmp_str_dict)\n",
    "        return self\n",
    "\n",
    "    def translate(self,mode='LLM',t_type='regular',**kwargs):\n",
    "        self.translation_type = t_type\n",
    "        cur_prompt = None\n",
    "        pred = None\n",
    "        if mode == 'LLM':\n",
    "            cur_output,cur_prompt,pred = translate_LLM(self,t_type=t_type,**kwargs)\n",
    "        elif mode == 'cache':\n",
    "            cur_output = get_translate_cache(self,t_type=t_type)\n",
    "        elif mode == 'NoRun' and t_type == 'template':\n",
    "            cur_output = self.template_translation\n",
    "        elif mode == 'NoRun' and t_type == 'regular':\n",
    "            cur_output = self.translation\n",
    "        else:\n",
    "            assert False, \"translation mode not found! \" + mode\n",
    "        \n",
    "        if cur_output is None:\n",
    "            cur_template_translation = None\n",
    "            cur_translation = None\n",
    "        elif t_type == 'template':\n",
    "            cur_template_translation = cur_output\n",
    "            cur_translation = cur_output\n",
    "            for dcmp_var,dcmp_node in self.dcmp_dict.items():\n",
    "                if dcmp_node.translation is not None:\n",
    "                    cur_translation = cur_translation.replace(\"_\"+dcmp_var+\"_\",\"(\"+dcmp_node.translation+\")\")\n",
    "        elif t_type == 'regular':\n",
    "            cur_template_translation = None\n",
    "            cur_translation = cur_output\n",
    "        self.translation = cur_translation\n",
    "        self.template_translation = cur_template_translation\n",
    "        return cur_prompt, pred\n",
    "\n",
    "    def check(self,DUT_formula,ret_trace=False,ret_trace_formula=False):\n",
    "        if self.translation is None:\n",
    "            return False, None\n",
    "        elif not spot_utils.check_wellformed(self.translation):\n",
    "            return False, None\n",
    "\n",
    "        cur_conjuct = get_conjucts_for_node(self,debug=False)\n",
    "        if not spot_utils.check_wellformed(cur_conjuct):\n",
    "            #print(\"WARNING: this node is not used by the final translation!\")\n",
    "            return False, None\n",
    "        \n",
    "        if spot_utils.check_formula_contains_formula(cur_conjuct,DUT_formula,use_contains_split=True):\n",
    "            return True, None\n",
    "        else:\n",
    "            if ret_trace:\n",
    "                #print(\"ATTEMPTING TO GET COUNTER EXAMPLE\")\n",
    "                trace = spot_utils.get_counter_example(DUT_formula,cur_conjuct,ret_trace_formula=ret_trace_formula)\n",
    "                #print(\"FINISHED\")\n",
    "                return False, trace\n",
    "            else:\n",
    "                return False, None\n",
    "\n",
    "    def clear(self):\n",
    "        self.translation = None\n",
    "        self.translation_type = None\n",
    "        self.template_translation = None\n",
    "        self.dcmp_dict = {}\n",
    "\n",
    "def get_root(node):\n",
    "    if node.parent is None:\n",
    "        return node\n",
    "    else:\n",
    "        return get_root(node.parent)\n",
    "\n",
    "def get_node_id(node):\n",
    "    assert False, \"deprecated\"\n",
    "    node_id = node.assert_text\n",
    "    for dcmp_var,dcmp_node in node.dcmp_dict.items():\n",
    "        node_id = node_id.replace(dcmp_node.assert_text,\"_\"+dcmp_var+\"_\")\n",
    "    return node_id\n",
    "\n",
    "def get_node_assert_text(node):\n",
    "    assert False, \"deprecated\"\n",
    "    return node.assert_text\n",
    "\n",
    "def get_node_translation(node):\n",
    "    if len(node.dcmp_dict) == 0:\n",
    "        return node.translation\n",
    "    else:\n",
    "        return node.template_translation\n",
    "\n",
    "def find_descendant_by_id(node,targ_node_id,get_node_id_func):\n",
    "    if get_node_id_func(node) == targ_node_id:\n",
    "        return node\n",
    "    for dcmp_var,dcmp_node in node.dcmp_dict.items():\n",
    "        desc_node = find_descendant_by_id(dcmp_node,targ_node_id,get_node_id_func=get_node_id_func)\n",
    "        if desc_node is not None:\n",
    "            return desc_node\n",
    "    return None    \n",
    "\n",
    "def find_descendant(node,targ_node_id):\n",
    "    if get_unique_node_id(node) == targ_node_id:\n",
    "        return node\n",
    "    for dcmp_var,dcmp_node in node.dcmp_dict.items():\n",
    "        desc_node = find_descendant(dcmp_node,targ_node_id)\n",
    "        if desc_node is not None:\n",
    "            return desc_node\n",
    "    return None\n",
    "\n",
    "def copy_graph(node):\n",
    "    new_node = Node(assert_text=node.assert_text)\n",
    "    new_node.translation = node.translation\n",
    "    new_node.translation_type = node.translation_type\n",
    "    new_node.template_translation = node.template_translation\n",
    "    new_node.translate_fewshots = node.translate_fewshots\n",
    "    new_node.decompose_fewshots = node.decompose_fewshots\n",
    "    for dcmp_var,dcmp_node in node.dcmp_dict.items():\n",
    "        new_node.dcmp_dict[dcmp_var] = copy_graph(dcmp_node)\n",
    "        new_node.dcmp_dict[dcmp_var].parent = new_node\n",
    "    return new_node\n",
    "\n",
    "def get_conjucts_for_node(node,verbose=False,ret_list=False,debug=True,omit_trivial=True,depth=None):\n",
    "    tmp_dcmp_dict = node.dcmp_dict\n",
    "    tmp_template_translation = node.template_translation\n",
    "    tmp_translation = node.translation\n",
    "    cur_root = get_root(node)\n",
    "    all_translation = cur_root.translation\n",
    "    \n",
    "    node_identifier = \"specialAP\"\n",
    "    assert node_identifier not in cur_root.translation, cur_root.translation\n",
    "    node.dcmp_dict = {}\n",
    "    node.template_translation = None\n",
    "    node.translation = node_identifier\n",
    "\n",
    "    dfs_translate(cur_root,mode='NoRun',t_type='template')\n",
    "    all_conjucts = spot_utils.get_conjucts(cur_root.translation,depth=depth)\n",
    "    abstract_conjucts_list = [prop for prop in all_conjucts if node_identifier in prop]\n",
    "    conjucts_for_node = \" && \".join(abstract_conjucts_list)\n",
    "    res = conjucts_for_node.replace(node_identifier,\"(\"+tmp_translation+\")\")\n",
    "    #abstract_conjucts_list = [prop.replace(node_identifier,\"(\"+tmp_translation+\")\") for prop in all_conjucts if node_identifier in prop]\n",
    "    #res = \" && \".join(abstract_conjucts_list)\n",
    "    node.dcmp_dict = tmp_dcmp_dict\n",
    "    node.template_translation = tmp_template_translation\n",
    "    node.translation = tmp_translation\n",
    "    dfs_translate(cur_root,mode='NoRun',t_type='template')\n",
    "    assert not debug or spot_utils.check_equivalent(\" && \".join(all_conjucts).replace(node_identifier,\"(\"+tmp_translation+\")\"),all_translation)\n",
    "    if not ret_list:\n",
    "        return res\n",
    "    else:\n",
    "        conjucts_for_node_list = [clause for clause in spot_utils.get_conjucts(res) if not omit_trivial or not spot_utils.check_equivalent(\"1\",clause)]\n",
    "        return conjucts_for_node_list\n",
    "\n",
    "def get_disjuncts_for_node(node,verbose=False,ret_list=False):\n",
    "    tmp_dcmp_dict = node.dcmp_dict\n",
    "    tmp_template_translation = node.template_translation\n",
    "    tmp_translation = node.translation\n",
    "    cur_root = get_root(node)\n",
    "    all_translation = cur_root.translation\n",
    "    \n",
    "    node_identifier = \"specialAP\"\n",
    "    assert node_identifier not in cur_root.translation, cur_root.translation\n",
    "    node.dcmp_dict = {}\n",
    "    node.template_translation = None\n",
    "    node.translation = node_identifier\n",
    "\n",
    "    dfs_translate(cur_root,mode='NoRun',t_type='template')\n",
    "    all_disjuncts = spot_utils.get_disjuncts(cur_root.translation)\n",
    "    abstract_disjuncts_list = [prop for prop in all_disjuncts if node_identifier in prop]\n",
    "    disjuncts_for_node = \" | \".join(abstract_disjuncts_list)\n",
    "    res = disjuncts_for_node.replace(node_identifier,\"(\"+tmp_translation+\")\")\n",
    "    #abstract_disjuncts_list = [prop.replace(node_identifier,\"(\"+tmp_translation+\")\") for prop in all_disjuncts if node_identifier in prop]\n",
    "    #res = \" && \".join(abstract_disjuncts_list)\n",
    "    \n",
    "    node.dcmp_dict = tmp_dcmp_dict\n",
    "    node.template_translation = tmp_template_translation\n",
    "    node.translation = tmp_translation\n",
    "    dfs_translate(cur_root,mode='NoRun',t_type='template')\n",
    "    try:\n",
    "        is_equal = spot_utils.check_equivalent(\" | \".join(all_disjuncts).replace(node_identifier,\"(\"+tmp_translation+\")\"),all_translation)\n",
    "    except:\n",
    "        is_equal = True\n",
    "        print(\"WARNING: exception thrown when verifying get_disjuncts_for_node\")\n",
    "    assert is_equal\n",
    "    if not ret_list:\n",
    "        return res\n",
    "    else:\n",
    "        if len(abstract_disjuncts_list) > 0:\n",
    "            disjuncts_for_node_list = [clause for clause in spot_utils.get_disjuncts(res) if not spot_utils.check_equivalent(\"0\",clause)]\n",
    "            return disjuncts_for_node_list\n",
    "        else:\n",
    "            return []\n",
    "\n",
    "def get_all_ancestors(node,inclusive=True):\n",
    "    if inclusive:\n",
    "        cur_list = [node]\n",
    "    else:\n",
    "        cur_list = []\n",
    "    if node.parent is not None:\n",
    "        cur_list += get_all_ancestors(node.parent,inclusive=True)\n",
    "    return cur_list\n",
    "\n",
    "def get_all_descendants(node,inclusive=True):\n",
    "    if inclusive:\n",
    "        cur_list = [node]\n",
    "    else:\n",
    "        cur_list = []\n",
    "    for dcmp_var,dcmp_node in node.dcmp_dict.items():\n",
    "        cur_list += get_all_descendants(dcmp_node,inclusive=True)\n",
    "    return cur_list\n",
    "\n",
    "def dfs_decompose(node,mode='LLM',max_depth=None,**kwargs):\n",
    "    if max_depth is not None and max_depth == 0:\n",
    "        return node\n",
    "    node.decompose(mode,**kwargs)\n",
    "    if max_depth is not None:\n",
    "        next_max_depth = max_depth-1\n",
    "    else:\n",
    "        next_max_depth = None\n",
    "    for dcmp_var,dcmp_node in node.dcmp_dict.items():\n",
    "        dfs_decompose(dcmp_node,mode=mode,max_depth=next_max_depth,**kwargs)\n",
    "    return node\n",
    "\n",
    "def dfs_translate(node,mode='LLM',t_type='regular',**kwargs):\n",
    "    for dcmp_var,dcmp_node in node.dcmp_dict.items():\n",
    "        dfs_translate(dcmp_node,mode=mode,t_type=t_type,**kwargs)\n",
    "    if len(node.dcmp_dict.items()) == 0:\n",
    "        node.translate(mode,t_type='regular',**kwargs)\n",
    "    else:\n",
    "        node.translate(mode,t_type=t_type,**kwargs)\n",
    "\n",
    "def get_translate_cache(node,t_type='regular'):\n",
    "    if t_type == 'regular':\n",
    "        assert_text = node.assert_text\n",
    "    elif t_type == 'template':\n",
    "        abs_node_id = get_abstract_node_id(node)\n",
    "        assert_text = abs_node_id\n",
    "        assert assert_text in translate_dict, assert_text\n",
    "    else:\n",
    "        assert False\n",
    "    if assert_text in translate_dict:\n",
    "        res = translate_dict[assert_text]\n",
    "    else:\n",
    "        print(\"WARNING:\",assert_text,\"not in cached translations!\")\n",
    "        res = None\n",
    "    return res\n",
    "\n",
    "def get_decompose_cache(node):\n",
    "    if node.assert_text in decompose_dict:\n",
    "        dcmp_dict = decompose_dict[node.assert_text]\n",
    "        res = dict((k,v) for k,v in dcmp_dict.items() if v != \"\" and v != node.assert_text)\n",
    "    else:\n",
    "        print(\"WARNING:\",node.assert_text,\"not in cached decompositions!\")\n",
    "        res = {}\n",
    "    return res\n",
    "\n",
    "def get_max_depth(node):\n",
    "    cur_max = 0\n",
    "    for dcmp_var,dcmp_node in node.dcmp_dict.items():\n",
    "        cur_max = max(cur_max,get_max_depth(dcmp_node))\n",
    "    return 1 + cur_max\n",
    "\n",
    "def get_nodes_by_depth(cur_graph,depth,include_leaves=False):\n",
    "    if include_leaves and (depth==0 or len(cur_graph.dcmp_dict) == 0):\n",
    "        return [cur_graph]\n",
    "    elif not include_leaves and depth==0:\n",
    "        return [cur_graph]\n",
    "    else:\n",
    "        res_list = []\n",
    "        for dcmp_var,dcmp_node in cur_graph.dcmp_dict.items():\n",
    "            res_list += get_nodes_by_depth(dcmp_node,depth-1,include_leaves=include_leaves)\n",
    "        return res_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72c08ae8-280f-419c-be6b-99dd2ea0ec90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Levenshtein\n",
    "import spot\n",
    "def helper_normalize_symbols(formula):\n",
    "    map = {\n",
    "            \"&&\":\"&\",\n",
    "            \"||\" : \"|\",\n",
    "            \"<->\" : \"=\",\n",
    "            \"->\" : \">\",\n",
    "          }\n",
    "    for k,v in map.items():\n",
    "        formula = formula.replace(k,v)\n",
    "    return \"\".join(formula.split())\n",
    "    \n",
    "def get_levenshtein_distance_nonformed(formula_a,formula_b):\n",
    "    try:\n",
    "        f_a = str(spot.formula(formula_a))\n",
    "        f_b = str(spot.formula(formula_b))\n",
    "    except:\n",
    "        f_a = formula_a\n",
    "        f_b = formula_b\n",
    "    var_set = set()\n",
    "    var_set.update(set(re.findall('_[a-zA-Z0-9_]*_',formula_a)))\n",
    "    var_set.update(set(re.findall('_[a-zA-Z0-9_]*_',formula_b)))\n",
    "    i = 0\n",
    "    repl_list = list(set(string.ascii_lowercase+string.ascii_uppercase)-var_set)\n",
    "    assert len(repl_list) > 0\n",
    "    assert len(var_set) <= len(string.ascii_lowercase+string.ascii_uppercase)\n",
    "    for var in var_set:\n",
    "        f_a = f_a.replace(var,repl_list[i])\n",
    "        f_b = f_b.replace(var,repl_list[i])\n",
    "        i += 1\n",
    "    f_a = helper_normalize_symbols(f_a)\n",
    "    f_b = helper_normalize_symbols(f_b)\n",
    "    return Levenshtein.distance(f_a,f_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c638ddf3-a46c-475e-b5a7-804bb27fa9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_nl2spec_fewshots(fname):\n",
    "    res = []\n",
    "    decompose_fewshots = pd.read_excel(fname).to_dict('records')\n",
    "    for entry in decompose_fewshots:\n",
    "        subtranslation_list = ast.literal_eval(entry[\"sub-translations\"])\n",
    "        for subtranslation in subtranslation_list:\n",
    "            #print(subtranslation)\n",
    "            assert spot_utils.check_wellformed(subtranslation[-1])\n",
    "        assert spot_utils.check_wellformed(entry[\"LTL\"])\n",
    "        cur_entry = entry.copy()\n",
    "        cur_entry[\"sub-translations\"] = [(sub[0],str(spot.formula(sub[1]))) for sub in subtranslation_list]\n",
    "        res.append(cur_entry)\n",
    "    return res\n",
    "decompose_fewshots = read_nl2spec_fewshots('nl2spec_fewshots.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61cb5337-8edd-4e74-915f-37c9195d06b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "nl2spec_prefix_prompt = \\\n",
    "\"\"\"\n",
    "You are a Linear Temporal Logic (LTL) expert. Your answers always need to follow the following output format. \n",
    "Decompose the following natural language sentences into phrases that can be independently translated to an LTL formula.\n",
    "Remember that X means \"next\", U means \"until\", G means \"globally\", F means \"finally\", which means GF means \"infinitely often\".\n",
    "The formula should only contain atomic propositions or operators ||, &&, !, ->, <->, X, U, G, F.\n",
    "\n",
    "The following explain each field of the translation:\n",
    "Natural language: the natural language phrase to be decomposed into sub-translations and translated to LTL. The Natural language field must contain all of the input natural language text for the current translation.\n",
    "sub-translations: a list of tuples which map sub-phrases of the Natural language and to the phrase's corresponding formalization in LTL.\n",
    "LTL: the LTL formula which captures the intended formalization of the entire Natural language. The sub-translations should be used to compose the full LTL formula. Note that the LTL formula should be well-formed.\n",
    "\"\"\"\n",
    "\n",
    "nl2spec_prefix_prompt += \"\\nAccount for the following example translations:\\n\"\n",
    "for ex in decompose_fewshots:\n",
    "    nl2spec_prefix_prompt += \"\\n\" +  str(ex) + \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1d948c26-1520-46c8-a39d-e8ffcea8486f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_str = open(\"specs/amba_master_godhal.txt\").read()\n",
    "#test_str = open(\"specs/amba_slave_godhal.txt\").read()\n",
    "test_str = open(\"specs/amba_arbiter_godhal.txt\").read()\n",
    "#test_str = \"if the memory is empty and a read transfer is attempted, then the slave shall send an ERROR response.\"\n",
    "#test_str = 'G1 When the slave is not selected by the decoder, HREADY signal shall be high.\\nG2 When the slave is not selected by the decoder, HRESP shall be OKAY.\\nG3 When no AHB transaction is taking place, HRESP shall be OKAY.\\nG4 RD and WR signal cannot be high simultaneously.\\nG5 If memory is full and write transfer is attempted, then the slave shall send an ERROR response. Similarly, if the memory is empty and a read transfer is attempted, then the slave shall send an ERROR response.\\nG6 When slave is involved in a transfer, HWRITE is used to decide values of WR and RD.\\nG7 When slave is involved in any transfer, signal HADDR is used to decide ADDR.\\nG8 When slave is involved in write transfer, signal HWDATA is used to decide DI.\\nG9 When slave is involved in read transfer, signal DO is used to decide HRDATA.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "66aec2ca-4d36-474e-9e60-b3a7d5266415",
   "metadata": {},
   "outputs": [],
   "source": [
    "decompose_dict = create_decompose_dict('rawcontext_decomposition-arbiter_v3.xlsx')\n",
    "translate_dict = create_translate_dict('rawcontext_decomposition-arbiter_v3.xlsx')\n",
    "#decompose_dict = create_decompose_dict('rawcontext_decomposition-master.xlsx')\n",
    "#translate_dict = create_translate_dict('rawcontext_decomposition-master.xlsx')\n",
    "#decompose_dict = create_decompose_dict('rawcontext_decomposition-slave.xlsx')\n",
    "#translate_dict = create_translate_dict('rawcontext_decomposition-slave.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "306eef2f-317d-491a-9f5f-1b4901067365",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.32 ms, sys: 0 ns, total: 3.32 ms\n",
      "Wall time: 3.19 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "formula_DUT = translate_dict[test_str]\n",
    "cur_graph = Node(test_str)\n",
    "dfs_decompose(cur_graph,mode='cache')\n",
    "dfs_translate(cur_graph,mode='cache',t_type='template')\n",
    "#assert spot_utils.check_equivalent(cur_graph.translation,formula_DUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "27d79c05-e0cd-48a6-b14e-ce7b6754c49d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_max_depth(cur_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "926a7114-0b38-4ee1-bf54-9e3b17ffbcad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(get_all_descendants(cur_graph))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "01f117cc-4215-407a-bcec-09ae2ac9b7b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(get_nodes_by_depth(cur_graph,depth=get_max_depth(cur_graph)-1,include_leaves=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9cb38c21-fc1b-42d7-b4d8-52e5156c78a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_var_list = spot_utils.get_variables(formula_DUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "895b7d1c-eab4-4227-9b6a-8b748865f747",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_nl2spec(nl_phrase,cur_subtranslations,prefix_prompt,num_try=1):\n",
    "    total_new_subtranslations = []\n",
    "    new_translation = \"\"\n",
    "    \n",
    "    cur_prompt = prefix_prompt\n",
    "    cur_prompt += \"\\nThe LTL translation may only contain a subset of the following variables: \"+str(cur_var_list)\n",
    "    cur_prompt += \"\\nProvide the LTL formula and sub-translations for the following natural language phrase (the translation must only use LTL operators and the format must be in JSON as shown in previous examples):\\n\"\n",
    "    \n",
    "    if len(cur_subtranslations) > 0:\n",
    "        cur_obj = {\"Natural language\":nl_phrase,\"sub-translations\":list(cur_subtranslations)}\n",
    "    else:\n",
    "        cur_obj = {\"Natural language\":nl_phrase}\n",
    "    cur_prompt += \"\\n\" + str(cur_obj)\n",
    "    for i in range(num_try):\n",
    "        response = get_inference_response(cur_prompt,\n",
    "                                          model=\"gpt-4-0125-preview\"\n",
    "                                          #model=\"gpt-3.5-turbo-0125\"\n",
    "                                         )\n",
    "        pred = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "        \n",
    "        #print(cur_prompt)\n",
    "        #print(pred)\n",
    "        try:\n",
    "            json_pred = json.loads(pred)\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            new_subtranslations = json_pred[\"sub-translations\"]\n",
    "            new_subtranslations = [tuple(entry) for entry in new_subtranslations if len(entry) == 2]\n",
    "        except:\n",
    "            new_subtranslations = []\n",
    "        total_new_subtranslations += new_subtranslations\n",
    "        try:\n",
    "            if new_translation == \"\":\n",
    "                new_translation = json_pred[\"LTL\"]\n",
    "        except:\n",
    "            pass\n",
    "    return total_new_subtranslations, new_translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dd1eefc8-4b34-4758-84fd-70ad82fae1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_oracle_new_subtranslations(subtranslations,correct_subtranslations,bad_subtranslations):\n",
    "    new_good_subtranslations = []\n",
    "    for entry in subtranslations:\n",
    "        if entry not in correct_subtranslations and entry not in bad_subtranslations:\n",
    "                print(\"sub-translation found:\")\n",
    "                print(entry[0])\n",
    "                print()\n",
    "                print(\"translation:\")\n",
    "                print(entry[1])\n",
    "                print()\n",
    "                \n",
    "                response = \"\"\n",
    "                while \"y\" not in response and \"n\" not in response:\n",
    "                    try:\n",
    "                        response = input(\"is this a correct translation? y/n\")\n",
    "                    except:\n",
    "                        response = \"\"\n",
    "                        print(\"failed to get input\")\n",
    "                if \"y\" in response:\n",
    "                    correct_subtranslations.add(entry)\n",
    "                    new_good_subtranslations.append(entry)\n",
    "                elif \"n\" in response:\n",
    "                    bad_subtranslations.add(entry)\n",
    "                else:\n",
    "                    assert False\n",
    "    return new_good_subtranslations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c24d2a8c-2d97-44b3-b83d-9f481fd26e04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                     | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|████████▊                                   | 1/5 [06:52<27:29, 412.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|█████████████████▌                          | 2/5 [12:43<18:48, 376.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 2\n",
      "OpenAI API returned an API Error: HTTP code 502 from API (<html>\n",
      "<head><title>502 Bad Gateway</title></head>\n",
      "<body>\n",
      "<center><h1>502 Bad Gateway</h1></center>\n",
      "<hr><center>cloudflare</center>\n",
      "</body>\n",
      "</html>\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████████████████████████▍                 | 3/5 [20:02<13:29, 404.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████████████████████████████████▏        | 4/5 [29:39<07:52, 472.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████| 5/5 [35:39<00:00, 427.88s/it]\n"
     ]
    }
   ],
   "source": [
    "num_try = 3\n",
    "\n",
    "oracle_translation_set = set()\n",
    "all_translation_set = set()\n",
    "\n",
    "correct_subtranslations = set()\n",
    "bad_subtranslations = set()\n",
    "cur_subtranslations = []\n",
    "for iter in tqdm(range(get_max_depth(cur_graph))):\n",
    "    prev_subtranslations = cur_subtranslations.copy()\n",
    "    \n",
    "    abs_node_set = set()\n",
    "    for i in range(get_max_depth(cur_graph)-iter):\n",
    "        abs_node_set.update(get_nodes_by_depth(cur_graph,depth=i,include_leaves=True))\n",
    "    cur_best_subtranslations = []\n",
    "    for node in abs_node_set:\n",
    "        cur_best_subtranslations.append((get_abstract_node_id(node),str(spot.formula(get_node_translation(node)))))\n",
    "        for dcmp_node in node.dcmp_dict.values():\n",
    "            if dcmp_node not in abs_node_set:\n",
    "                cur_best_subtranslations.append( (dcmp_node.assert_text,str(spot.formula(dcmp_node.translation))) )\n",
    "    if iter > 0:\n",
    "        for entry in cur_best_subtranslations:\n",
    "            if entry not in all_translation_set:\n",
    "                oracle_translation_set.add(entry)\n",
    "    #print(len(abs_node_set))\n",
    "    #print(len(cur_best_subtranslations))\n",
    "    print(iter,len(oracle_translation_set))\n",
    "    cur_subtranslations = list(correct_subtranslations) + cur_best_subtranslations\n",
    "    all_translation_set.update(set(cur_subtranslations))\n",
    "    new_subtranslations, new_translation = query_nl2spec(test_str,cur_subtranslations,nl2spec_prefix_prompt,num_try=num_try)\n",
    "    all_translation_set.update(set(new_subtranslations))\n",
    "    all_translation_set.add((test_str,new_translation))\n",
    "    if spot_utils.check_wellformed(new_translation) and spot_utils.check_equivalent(new_translation,formula_DUT,use_contains_split=True):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c159861c-42af-490d-b35d-e6dbe5e15d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM fail!\n"
     ]
    }
   ],
   "source": [
    "if not (spot_utils.check_wellformed(new_translation) and spot_utils.check_equivalent(new_translation,formula_DUT)):\n",
    "    oracle_translation_set.add((test_str,formula_DUT))\n",
    "    all_translation_set.add((test_str,formula_DUT))\n",
    "    print(\"LLM fail!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "82eecdf7-540a-41b2-b6e1-09e4c5d6c0e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "490"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_levenshtein_distance_nonformed(\"0\",str(spot.formula(formula_DUT)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "9b1411d5-c53e-440c-8716-60e26f3dcad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name = \"godhalarbiter_gpt4\"\n",
    "#save_nl2spec_data(exp_name,all_translation_set,oracle_translation_set)\n",
    "all_translation_set,oracle_translation_set = load_nl2spec_data(exp_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "1426ce93-19c5-4fb0-9918-d4af1d0a881c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(oracle_translation_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "6a648338-832b-406d-99d3-fc0238fe2060",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_translation_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "9323ad3a-b908-4114-ae5c-f55b946d6103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4254\n",
      "45.255319148936174\n",
      "110.97651046938704\n",
      "490\n"
     ]
    }
   ],
   "source": [
    "res_list = []\n",
    "for entry in all_translation_set:\n",
    "    res_list.append(get_levenshtein_distance_nonformed(\"0\",entry[-1]))\n",
    "print(np.sum(res_list))\n",
    "print(np.mean(res_list))\n",
    "print(np.std(res_list))\n",
    "print(np.max(res_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "8469558d-9665-4490-85aa-2ac95e82ce4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1374\n",
      "59.73913043478261\n",
      "113.34833474978008\n",
      "490\n"
     ]
    }
   ],
   "source": [
    "res_list = []\n",
    "for entry in oracle_translation_set:\n",
    "    res_list.append(get_levenshtein_distance_nonformed(\"0\",entry[-1]))\n",
    "print(np.sum(res_list))\n",
    "print(np.mean(res_list))\n",
    "print(np.std(res_list))\n",
    "print(np.max(res_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9ad4fb2e-dad0-4e2b-972b-5db02c6c0546",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_nl2spec_data(exp_name,all_translation_set,all_oracle_translation_set):\n",
    "    with open(exp_name+'_nl2spec_alltranslation.json', 'w') as f:\n",
    "        json.dump(list(all_translation_set), f)\n",
    "    with open(exp_name+'_nl2spec_oracletranslation.json', 'w') as f:\n",
    "        json.dump(list(oracle_translation_set), f)\n",
    "def load_nl2spec_data(exp_name):\n",
    "    all_translation_set = json.load(open(exp_name+'_nl2spec_alltranslation.json', 'r'))\n",
    "    oracle_translation_set = json.load(open(exp_name+'_nl2spec_oracletranslation.json', 'r'))\n",
    "    return all_translation_set,oracle_translation_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa44d282-11b6-463f-843a-b9b420bac371",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
